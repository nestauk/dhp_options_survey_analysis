{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading and processing survey data\n",
    "\n",
    "- Load and clean up the data\n",
    "- Visualise using embeddings\n",
    "- Categorise using GPT-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import openai\n",
    "\n",
    "import sentence_transformers\n",
    "from sentence_transformers import SentenceTransformer\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "import umap\n",
    "import hdbscan\n",
    "import altair as alt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to data\n",
    "FILEPATH = \"data/KK Copy of test data for DS on options issue map.xlsx\"\n",
    "SHEETS = [\n",
    "    \"g&p issues\",\n",
    "    \"g&p interventions\",\n",
    "    \"health issues\",\n",
    "    \"health interventions\",\n",
    "    \"inequality issues\",\n",
    "    \"inequality interventions\",\n",
    "]\n",
    "\n",
    "issue_cols = [f\"Q_issue_{i}\" for i in range(1, 11)]\n",
    "other_cols = [f\"q_other_{i}\" for i in range(1, 12)]\n",
    "intervention_cols = [f\"q_intervention_{i}\" for i in range(1, 11)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_table(data_df: pd.DataFrame, sheet_name: str) -> pd.DataFrame:\n",
    "    \"\"\"Process on table of survey data\"\"\"\n",
    "    if 'issues' in sheet_name:\n",
    "        cols = issue_cols+other_cols\n",
    "    else:\n",
    "        cols = intervention_cols\n",
    "    return (\n",
    "        data_df\n",
    "        .melt(value_vars=cols)\n",
    "        .rename(columns={\"variable\": \"question\"})\n",
    "        .assign(data_type=sheet_name)\n",
    "        .assign(policy_area=lambda x: x.data_type.str.split(\" \").str[0])\n",
    "        .dropna(subset=['value'])\n",
    "        .query(\"value != '-'\")\n",
    "    )\n",
    "\n",
    "def load_and_process_survey() -> pd.DataFrame:\n",
    "    \"\"\"Load and process all survey data\"\"\"\n",
    "    dfs = []\n",
    "    for sheet_name in SHEETS:\n",
    "        data_df = pd.read_excel(FILEPATH, sheet_name)\n",
    "        dfs.append(process_table(data_df, sheet_name))\n",
    "    return pd.concat(dfs, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "survey_df = load_and_process_survey()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "survey_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embed and visualise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors = model.encode(survey_df.value.tolist(), show_progress_bar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use umap to reduce dimensionality\n",
    "umap_embeddings = umap.UMAP(\n",
    "    n_neighbors=15,\n",
    "    n_components=25,\n",
    ").fit_transform(vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use hdbscan to cluster, and assing all points to a cluster\n",
    "cluster = hdbscan.HDBSCAN(\n",
    "    min_cluster_size=10,\n",
    "    metric='euclidean',\n",
    "    cluster_selection_method='eom',\n",
    "    prediction_data=True,\n",
    ").fit(umap_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use umap to reduce to 2-d\n",
    "umap_embeddings_2d = umap.UMAP(\n",
    "    n_neighbors=15,\n",
    "    n_components=2,\n",
    ").fit_transform(vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use altair to plot the clusters\n",
    "survey_viz_df = (\n",
    "    survey_df\n",
    "    .assign(cluster=cluster.labels_)\n",
    "    .assign(x=umap_embeddings_2d[:, 0])\n",
    "    .assign(y=umap_embeddings_2d[:, 1])    \n",
    ")\n",
    "\n",
    "fig = (\n",
    "    alt.Chart(survey_viz_df)\n",
    "    .mark_circle()\n",
    "    .encode(\n",
    "        x='x',\n",
    "        y='y',\n",
    "        color='cluster:N',\n",
    "        tooltip=['value', 'cluster'],\n",
    "    )\n",
    "    .interactive()\n",
    ")\n",
    "\n",
    "fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use tf-idf to get top words from each cluster\n",
    "# first, join up all values in each cluster\n",
    "cluster_df = (\n",
    "    survey_viz_df\n",
    "    .groupby('cluster')\n",
    "    .agg({'value': ' '.join})\n",
    "    .reset_index()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # then, get top words for each cluster\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from collections import defaultdict\n",
    "vectorizer = TfidfVectorizer(stop_words='english')\n",
    "tfidf_matrix = vectorizer.fit_transform(cluster_df.value.tolist())\n",
    "\n",
    "# Get feature names\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "# Create a dictionary to hold top words for each cluster\n",
    "top_words_per_cluster = defaultdict(list)\n",
    "\n",
    "# Number of top words you want to display per cluster\n",
    "n_top_words = 5\n",
    "\n",
    "# Iterate over each cluster and get top words\n",
    "for cluster_idx, tfidf_scores in enumerate(tfidf_matrix):\n",
    "    # Get indices of top n words within the cluster\n",
    "    top_word_indices = tfidf_scores.toarray()[0].argsort()[:-n_top_words - 1:-1]\n",
    "    \n",
    "    # Get the top words corresponding to the top indices\n",
    "    top_words = [feature_names[i] for i in top_word_indices]\n",
    "    \n",
    "    # Append the words to the dictionary\n",
    "    top_words_per_cluster[cluster_df.iloc[cluster_idx]['cluster']] = top_words\n",
    "\n",
    "# Print the top words for each cluster\n",
    "cluster_names = [f\"Cluster {cluster}: {', '.join(words)}\" for cluster, words in top_words_per_cluster.items()]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "survey_clustered_df = (\n",
    "    survey_viz_df\n",
    "    .assign(cluster_name=lambda x: x.cluster.map(dict(zip(top_words_per_cluster.keys(), cluster_names))))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "survey_clustered_df.to_csv(\"data/survey_clustered.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    survey_clustered_df\n",
    "    .groupby(['cluster_name', 'policy_area'])\n",
    "    .agg({'value': 'count'})\n",
    "    .reset_index()\n",
    ").to_csv(\"data/survey_clustered_counts.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "options",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
